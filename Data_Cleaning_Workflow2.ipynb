{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRA3DpdrRiJ5"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum, when, lower, expr, count,isnan,hour, dayofweek, month, year, dayofmonth,approx_count_distinct\n",
        "from pyspark.sql.functions import lower as lower_func\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "spark = SparkSession.builder.appName(\"sparksqlexample\").getOrCreate()\n",
        "print(\"Session created\")\n",
        "df = spark.read.csv(\"file:///home/hduser/Downloads/archive(7)/data.csv\",header=True,inferSchema=True,encoding=\"utf-8\")\n",
        "\n",
        "#Column dtatatypes\n",
        "df.dtypes\n",
        "\n",
        "# Mean, min, max, count, stddev\n",
        "df.describe().show()\n",
        "\n",
        "# Median\n",
        "df.select(expr(\"percentile_approx(fare_amount, 0.5)\").alias(\"median_fare\")).show()\n",
        "\n",
        "# Mode\n",
        "df.groupBy(\"fare_amount\").count().orderBy(col(\"count\").desc()).show(1)\n",
        "\n",
        "df = df.withColumn(\"pickup_hour\", hour(col(\"tpep_pickup_datetime\"))) \\\n",
        "       .withColumn(\"pickup_day\", dayofmonth(col(\"tpep_pickup_datetime\"))) \\\n",
        "       .withColumn(\"pickup_month\", month(col(\"tpep_pickup_datetime\"))) \\\n",
        "       .withColumn(\"pickup_weekday\", dayofweek(col(\"tpep_pickup_datetime\"))) \\\n",
        "       .withColumn(\"pickup_year\", year(col(\"tpep_pickup_datetime\")))\n",
        "\n",
        "# Dropoff datetime features (different column names)\n",
        "df = df.withColumn(\"dropoff_hour\", hour(col(\"tpep_dropoff_datetime\"))) \\\n",
        "       .withColumn(\"dropoff_day\", dayofmonth(col(\"tpep_dropoff_datetime\"))) \\\n",
        "       .withColumn(\"dropoff_month\", month(col(\"tpep_dropoff_datetime\"))) \\\n",
        "       .withColumn(\"dropoff_weekday\", dayofweek(col(\"tpep_dropoff_datetime\"))) \\\n",
        "       .withColumn(\"dropoff_year\", year(col(\"tpep_dropoff_datetime\")))\n",
        "\n",
        "df = df.drop(*[\n",
        "    \"tpep_pickup_datetime\",\n",
        "    \"tpep_dropoff_datetime\"\n",
        "])\n",
        "\n",
        "\n",
        "#Missing Values\n",
        "numeric_cols = [f.name for f in df.schema.fields if f.dataType.simpleString() in ('double','float','int')]\n",
        "exprs = []\n",
        "for c in df.columns:\n",
        "    dtype = [f.dataType.simpleString() for f in df.schema.fields if f.name == c][0]\n",
        "    if dtype in ('double', 'float', 'int'):\n",
        "        exprs.append(count(when(col(c).isNull() | isnan(c), c)).alias(c))\n",
        "    else:\n",
        "        exprs.append(count(when(col(c).isNull(), c)).alias(c))\n",
        "null_counts = df.select(exprs)\n",
        "null_counts_pd = null_counts.toPandas()\n",
        "#Heatmap\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.heatmap(null_counts_pd, annot=True, cmap=\"YlGnBu\", cbar=False)\n",
        "plt.title(\"Missing Values Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "#  Duplicates\n",
        "cols_to_check = [\"VendorID\", \"passenger_count\", \"trip_distance\", \"fare_amount\"]\n",
        "duplicate_count = df.count() - df.dropDuplicates(subset=cols_to_check).count()\n",
        "print(f\"Duplicate rows based on selected columns: {duplicate_count}\")\n",
        "\n",
        "# Outliers (numeric columns)\n",
        "# Using IQR method\n",
        "for col_name in numeric_cols:\n",
        "    quantiles = df.approxQuantile(col_name, [0.25, 0.75], 0.05)\n",
        "    Q1, Q3 = quantiles\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5*IQR\n",
        "    upper_bound = Q3 + 1.5*IQR\n",
        "    outliers = df.filter((col(col_name) < lower_bound) | (col(col_name) > upper_bound)).count()\n",
        "    print(f\"Outliers in {col_name}: {outliers}\")\n",
        "\n",
        "# Structural Errors\n",
        "# - Inconsistent labels\n",
        "# - Check string columns\n",
        "numeric_cols = [f.name for f in df.schema.fields if f.dataType.simpleString() in ('double','float','int')]\n",
        "non_numeric_cols = [f for f in df.columns if f not in numeric_cols]\n",
        "for c in non_numeric_cols:\n",
        "    distinct_values = df.select(c).distinct().count()\n",
        "    approx_values = df.select(approx_count_distinct(c)).collect()[0][0]\n",
        "    print(f\"Column '{c}' has {distinct_values} distinct values (approx: {approx_values})\")\n",
        "\n",
        "# Remove unwanted rows\n",
        "# Example: remove rows with negative fare_amount or trip_distance\n",
        "initial_count = df.count()\n",
        "print(f\"Initial number of rows: {initial_count}\")\n",
        "df_clean = df.filter((col(\"fare_amount\") > 0) & (col(\"trip_distance\") > 0))\n",
        "after_unwanted_removal = df_clean.count()\n",
        "print(f\"Rows removed due to unwanted values: {initial_count - after_unwanted_removal}\")\n",
        "\n",
        "# Handle missing values\n",
        "# Drop rows with missing essential columns\n",
        "essential_cols = [\"fare_amount\", \"trip_distance\", \"passenger_count\"]\n",
        "df_clean2 = df_clean.dropna(subset=essential_cols)\n",
        "after_missing_removal = df_clean2.count()\n",
        "print(f\"Rows removed due to missing essential values: {after_unwanted_removal - after_missing_removal}\")\n",
        "\n",
        "# Outlier handling (example: clip outliers instead of dropping)\n",
        "# Store counts if you drop instead of clip\n",
        "numeric_cols = [\"fare_amount\", \"trip_distance\", \"passenger_count\"]\n",
        "for c in numeric_cols:\n",
        "    Q1, Q3 = df_clean2.approxQuantile(c, [0.25, 0.75], 0.01)\n",
        "    IQR = Q3 - Q1\n",
        "    lower = Q1 - 1.5*IQR\n",
        "    upper = Q3 + 1.5*IQR\n",
        "    # Optional: drop outliers\n",
        "    before_outlier_removal = df_clean2.count()\n",
        "    df_clean2 = df_clean2.filter((col(c) >= lower) & (col(c) <= upper))\n",
        "    after_outlier_removal = df_clean2.count()\n",
        "    print(f\"Rows removed from {c} outliers: {before_outlier_removal - after_outlier_removal}\")\n",
        "\n",
        "\n",
        "# Correct inconsistent values\n",
        "# Example: lowercase categorical columns\n",
        "from pyspark.sql.functions import lower\n",
        "\n",
        "for c in non_numeric_cols:\n",
        "    df_clean2 = df_clean2.withColumn(c, lower(col(c)))\n",
        "    print(f\"Column: {c}\")\n",
        "    df_clean2.select(c).distinct().show(truncate=False)\n",
        "\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "payment_indexer = StringIndexer(\n",
        "    inputCol=\"payment_type\",\n",
        "    outputCol=\"payment_type_encoded\",\n",
        "    handleInvalid=\"keep\"\n",
        ")\n",
        "\n",
        "store_indexer = StringIndexer(\n",
        "    inputCol=\"store_and_fwd_flag\",\n",
        "    outputCol=\"store_and_fwd_flag_encoded\",\n",
        "    handleInvalid=\"keep\"\n",
        ")\n",
        "\n",
        "# Encode store_and_fwd_flag\n",
        "df_final = store_indexer.fit(df_clean2).transform(df_clean2)\n",
        "\n",
        "# Encode payment_type\n",
        "df_final = payment_indexer.fit(df_final).transform(df_final)\n",
        "#Drop the column\n",
        "df_final = df_final.drop(\"store_and_fwd_flag\")\n",
        "\n",
        "from pyspark.sql.functions import min, max, col\n",
        "\n",
        "scale_cols = [\n",
        "    \"fare_amount\",\n",
        "    \"trip_distance\",\n",
        "    \"passenger_count\",\n",
        "    \"pickup_hour\",\n",
        "    \"pickup_day\",\n",
        "    \"pickup_month\",\n",
        "    \"pickup_year\"\n",
        "]\n",
        "\n",
        "for c in scale_cols:\n",
        "    stats = df_final.agg(\n",
        "        min(col(c)).alias(\"min_val\"),\n",
        "        max(col(c)).alias(\"max_val\")\n",
        "    ).collect()[0]\n",
        "\n",
        "    min_val = stats[\"min_val\"]\n",
        "    max_val = stats[\"max_val\"]\n",
        "\n",
        "    df_final = df_final.withColumn(\n",
        "        f\"{c}_scaled\",\n",
        "        (col(c) - min_val) / (max_val - min_val)\n",
        "    )\n",
        "\n",
        "df_final.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"header\", True) \\\n",
        "    .csv(\"file:///home/hduser/23processed_data_csv\")\n",
        "\n",
        "\n",
        "# Your PySpark code here\n",
        "df_final.write.mode(\"overwrite\").option(\"header\", True).csv(\"file:///home/hduser/processed_data_final\")\n",
        "end_time = time.time()  # end timer\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n"
      ]
    }
  ]
}